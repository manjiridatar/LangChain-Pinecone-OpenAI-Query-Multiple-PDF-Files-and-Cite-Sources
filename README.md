# LangChain-Pinecone-OpenAI-Query-Multiple-PDF-Files-and-Cite-Sources
LangChain Pinecone OpenAI - Query Multiple PDF Files and Cite Sources

This [notebook](https://github.com/manjiridatar/LangChain-Pinecone-OpenAI-Query-Multiple-PDF-Files-and-Cite-Sources/blob/main/LangChain%20Pinecone%20OpenAI%20-%20Query%20Your%20Own%20Files%20-%20Multiple%20PDFs%20with%20Sources.ipynb) guides you through the basics of loading multiple PDF file externally into Pinecone as embeddings(vectors).

It also guides you on the basics of querying your custom PDF files data to get answers back (semantic search) from the Pinecone vector database, via the OpenAI LLM API. We walk through 2 approaches, first using the RetrievalQA chain and the second using VectorStoreAgent

Using LLMs to query your own data is a powerful application to become operationally efficient for various tasks requiring looking up large documents.

[Watch the YouTube Tutorial Video](https://youtu.be/8R2ArvI9vkk)

As promised, these are the links to the Generative AI Whitepapers from [KPMG](https://assets.kpmg.com/content/dam/kpmg/xx/pdf/2023/04/generative-ai-models-the-risks-and-potential-rewards-in-business.pdf), [Accenture](https://www.accenture.com/content/dam/accenture/final/accenture-com/document/Accenture-A-New-Era-of-Generative-AI-for-Everyone.pdf), [McKinsey](https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/the%20economic%20potential%20of%20generative%20ai%20the%20next%20productivity%20frontier/the-economic-potential-of-generative-ai-the-next-productivity-frontier-vf.pdf)

